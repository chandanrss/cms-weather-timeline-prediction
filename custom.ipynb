{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923ccf1a",
   "metadata": {},
   "source": [
    "Reading Task Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e365082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV files\n",
    "tasks_df = pd.read_csv('data/tasks.csv')\n",
    "change_orders_df = pd.read_csv('data/change_orders.csv')\n",
    "risk_events_df = pd.read_csv('data/risk_events.csv')\n",
    "inspections_df = pd.read_csv('data/inspection_records.csv')\n",
    "weather_history_df = pd.read_csv('data/weather_history.csv')\n",
    "weather_history_2024_df = pd.read_csv('data/weather_history_2024.csv')\n",
    "# Display the first few rows of each for confirmation\n",
    "# print(\"Tasks Data:\")\n",
    "# print(tasks_df.head(), \"\\n\")\n",
    "\n",
    "# print(\"Change Orders Data:\")\n",
    "# print(change_orders_df.head(), \"\\n\")\n",
    "\n",
    "# print(\"Risk Events Data:\")\n",
    "# print(risk_events_df.head(), \"\\n\")\n",
    "\n",
    "# print(\"Inspection Records Data:\")\n",
    "# print(inspections_df.head(), \"\\n\") \n",
    "\n",
    "# print(\"Weather History Data:\")\n",
    "# print(weather_history_df.head(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a30ccc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aef64bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  task_id               task_name  priority_score\n",
      "0      T1              Excavation              15\n",
      "1      T2         Soil Compaction              13\n",
      "2      T3         Foundation Pour              18\n",
      "3      T4  Basement Waterproofing              12\n",
      "4      T5             Backfilling              11\n",
      "5      T6            Slab Casting              13\n",
      "6      T7     Ground Floor Column               8\n",
      "7      T8      Ground Floor Walls               9\n",
      "8      T9               Roof Slab              10\n"
     ]
    }
   ],
   "source": [
    "# Compute priority score\n",
    "tasks_df['priority_score'] = tasks_df['duration_days'] + tasks_df['type_weight'] + tasks_df['weather_weight']\n",
    "\n",
    "# Sort tasks by priority (descending)\n",
    "# df_sorted = tasks_df.sort_values(by='priority_score', ascending=False)\n",
    "\n",
    "# Show the priority list\n",
    "print(tasks_df[['task_id', 'task_name', 'priority_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab464f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Change order C1 applied to task T3. Duration increased by 2 days.\n",
      "✅ Change order C2 applied to task T6. Duration increased by 1 days.\n",
      "✅ Change order C3 applied to task T9. Duration increased by 2 days.\n",
      "Processing task T1 with duration 7 days and dependency nan\n",
      "Adjusted duration for task T1 is now 10 days due to risk score impact of 0.7\n",
      "Adjusted duration for task T1 is now 10 days due to inspection failures\n",
      "-------------------------------------\n",
      "Processing task T2 with duration 6 days and dependency T1\n",
      "Adjusted duration for task T2 is now 6 days due to risk score impact of None\n",
      "Adjusted duration for task T2 is now 6 days due to inspection failures\n",
      "-------------------------------------\n",
      "Processing task T3 with duration 11 days and dependency T2\n",
      "Adjusted duration for task T3 is now 13 days due to risk score impact of 0.6\n",
      "Adjusted duration for task T3 is now 15 days due to inspection failures\n",
      "-------------------------------------\n",
      "Processing task T4 with duration 5 days and dependency T3\n",
      "Adjusted duration for task T4 is now 5 days due to risk score impact of None\n",
      "Adjusted duration for task T4 is now 5 days due to inspection failures\n",
      "-------------------------------------\n",
      "Processing task T5 with duration 4 days and dependency T4\n",
      "Adjusted duration for task T5 is now 6 days due to risk score impact of 0.5\n",
      "Adjusted duration for task T5 is now 6 days due to inspection failures\n",
      "-------------------------------------\n",
      "Processing task T6 with duration 7 days and dependency T5\n",
      "Adjusted duration for task T6 is now 7 days due to risk score impact of None\n",
      "Adjusted duration for task T6 is now 7 days due to inspection failures\n",
      "-------------------------------------\n",
      "Processing task T7 with duration 4 days and dependency T6\n",
      "Adjusted duration for task T7 is now 4 days due to risk score impact of None\n",
      "Adjusted duration for task T7 is now 4 days due to inspection failures\n",
      "-------------------------------------\n",
      "Processing task T8 with duration 5 days and dependency T7\n",
      "Adjusted duration for task T8 is now 5 days due to risk score impact of None\n",
      "Adjusted duration for task T8 is now 5 days due to inspection failures\n",
      "-------------------------------------\n",
      "Processing task T9 with duration 9 days and dependency T8\n",
      "Adjusted duration for task T9 is now 10 days due to risk score impact of 0.4\n",
      "Adjusted duration for task T9 is now 10 days due to inspection failures\n",
      "-------------------------------------\n",
      "Schedule computed successfully. {'T1': {'start_date': Timestamp('2024-05-01 00:00:00'), 'end_date': Timestamp('2024-05-10 00:00:00'), 'risk_score': 0.7, 'adjusted_duration': 10}, 'T2': {'start_date': Timestamp('2024-05-11 00:00:00'), 'end_date': Timestamp('2024-05-16 00:00:00'), 'risk_score': None, 'adjusted_duration': 6}, 'T3': {'start_date': Timestamp('2024-05-17 00:00:00'), 'end_date': Timestamp('2024-05-31 00:00:00'), 'risk_score': 0.6, 'adjusted_duration': 15}, 'T4': {'start_date': Timestamp('2024-06-01 00:00:00'), 'end_date': Timestamp('2024-06-05 00:00:00'), 'risk_score': None, 'adjusted_duration': 5}, 'T5': {'start_date': Timestamp('2024-06-06 00:00:00'), 'end_date': Timestamp('2024-06-11 00:00:00'), 'risk_score': 0.5, 'adjusted_duration': 6}, 'T6': {'start_date': Timestamp('2024-06-12 00:00:00'), 'end_date': Timestamp('2024-06-18 00:00:00'), 'risk_score': None, 'adjusted_duration': 7}, 'T7': {'start_date': Timestamp('2024-06-19 00:00:00'), 'end_date': Timestamp('2024-06-22 00:00:00'), 'risk_score': None, 'adjusted_duration': 4}, 'T8': {'start_date': Timestamp('2024-06-23 00:00:00'), 'end_date': Timestamp('2024-06-27 00:00:00'), 'risk_score': None, 'adjusted_duration': 5}, 'T9': {'start_date': Timestamp('2024-06-28 00:00:00'), 'end_date': Timestamp('2024-07-07 00:00:00'), 'risk_score': 0.4, 'adjusted_duration': 10}}\n",
      "✅ Updated task list with change orders applied:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# Set project_start if not already defined\n",
    "project_start = pd.to_datetime(tasks_df['start_date']).min()\n",
    "\n",
    "# Step 1: Load CSVs\n",
    "changes_df = change_orders_df\n",
    "\n",
    "# Step 2: Apply changes based on change orders\n",
    "for _, change in changes_df.iterrows():\n",
    "    task_id = change['task_id']\n",
    "    added_duration = change['added_duration']\n",
    "\n",
    "    # Check if task exists\n",
    "    if task_id in tasks_df['task_id'].values:\n",
    "        # Update the duration\n",
    "        tasks_df.loc[tasks_df['task_id'] == task_id, 'duration_days'] += added_duration\n",
    "\n",
    "        # Update the cost impact\n",
    "        tasks_df.loc[tasks_df['task_id'] == task_id, 'actual_cost'] += change['cost_impact']\n",
    "\n",
    "        # Optional: mark that a change has been applied\n",
    "        tasks_df.loc[tasks_df['task_id'] == task_id, 'change_applied'] = change['change_id']\n",
    "        print(f\"✅ Change order {change['change_id']} applied to task {task_id}. Duration increased by {added_duration} days.\")\n",
    "    else:\n",
    "        print(f\"Warning: Task {task_id} in change order not found in tasks list.\")\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# Function to compute start and end dates considering dependencies\n",
    "def compute_schedule(df, start_date):\n",
    "    schedule = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        dep = row['depends_on']\n",
    "        duration = row['duration_days']\n",
    "        risk_score = None\n",
    "\n",
    "        print(f\"Processing task {row['task_id']} with duration {duration} days and dependency {dep}\")\n",
    "        \n",
    "        # Apply risk score impact\n",
    "        risk = risk_events_df[risk_events_df['task_id'] == row['task_id']]\n",
    "        if not risk.empty:\n",
    "            risk_score = float(risk.iloc[0]['risk_score'])\n",
    "            if risk_score >= 0.7:\n",
    "                duration += 3\n",
    "            elif risk_score >= 0.5:\n",
    "                duration += 2\n",
    "            elif risk_score >= 0.3:\n",
    "                duration += 1\n",
    "\n",
    "        print(f\"Adjusted duration for task {row['task_id']} is now {duration} days due to risk score impact of {risk_score}\")\n",
    "\n",
    "        # Apply inspection failures\n",
    "        inspection = inspections_df[(inspections_df['task_id'] == row['task_id']) & (inspections_df['passed'] == 0)]\n",
    "        if not inspection.empty:\n",
    "            duration += 2  # inspection failed\n",
    "\n",
    "        print(f\"Adjusted duration for task {row['task_id']} is now {duration} days due to inspection failures\")\n",
    "        print(\"-------------------------------------\")\n",
    "\n",
    "        if pd.isna(dep):\n",
    "            s_date = start_date\n",
    "        else:\n",
    "            s_date = schedule[dep]['end_date'] + timedelta(days=1)\n",
    "        e_date = s_date + timedelta(days=duration - 1)\n",
    "        schedule[row['task_id']] = {\n",
    "            'start_date': s_date,\n",
    "            'end_date': e_date,\n",
    "            'risk_score': risk_score,\n",
    "            'adjusted_duration': duration\n",
    "        }\n",
    "    return schedule\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "schedule_map = compute_schedule(tasks_df, project_start)\n",
    "print(\"Schedule computed successfully.\", schedule_map)\n",
    "\n",
    "# Add schedule to tasks_df\n",
    "tasks_df['start_date'] = tasks_df['task_id'].apply(lambda x: schedule_map[x]['start_date'])\n",
    "tasks_df['end_date'] = tasks_df['task_id'].apply(lambda x: schedule_map[x]['end_date'])\n",
    "tasks_df['risk_score'] = tasks_df['task_id'].apply(lambda x: schedule_map[x]['risk_score'])\n",
    "tasks_df['adjusted_duration'] = tasks_df['task_id'].apply(lambda x: schedule_map[x]['adjusted_duration'])\n",
    "\n",
    "# Step 3: Fill NaN in 'change_applied' with 'None'\n",
    "tasks_df['change_applied'] = tasks_df['change_applied'].fillna('None')\n",
    "\n",
    "# Step 4: Save updated tasks\n",
    "tasks_df.to_csv('result/updated_tasks.csv', index=False)\n",
    "\n",
    "# Step 5: Print result\n",
    "print(\"✅ Updated task list with change orders applied:\\n\")\n",
    "# print(tasks_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df79e29",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0128dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Weather Data with Delays:\n",
      "         date  temp_mean  weather_code_mode  rain_sum  precipitation_mm  \\\n",
      "0  2024-01-01   6.216667                  3       0.4          0.025000   \n",
      "1  2024-01-02   4.804167                 61       7.6          0.316667   \n",
      "2  2024-01-03   9.212500                  3       4.1          0.175000   \n",
      "3  2024-01-04   4.170833                  3       2.1          0.095833   \n",
      "4  2024-01-05   0.541667                  3       1.3          0.120833   \n",
      "\n",
      "   wind_mean  delay  \n",
      "0   9.695833      0  \n",
      "1   9.716667      0  \n",
      "2  14.883333      0  \n",
      "3  14.062500      0  \n",
      "4  11.166667      1  \n"
     ]
    }
   ],
   "source": [
    "# Ensure date columns are datetime\n",
    "weather_df = weather_history_2024_df\n",
    "weather_df['date'] = pd.to_datetime(weather_df['time'])\n",
    "# print(tasks_df['start_date'], weather_df['date'])\n",
    "\n",
    "# Step 2: Aggregate by date\n",
    "daily_df = weather_df.groupby(weather_df['date'].dt.date).agg({\n",
    "    'temperature_2m (°C)': 'mean',\n",
    "    'weather_code (wmo code)': lambda x: x.mode()[0] if not x.mode().empty else x.iloc[0],\n",
    "    'rain (mm)': 'sum',\n",
    "    # 'snowfall (cm)': 'sum',\n",
    "    'precipitation_mm': 'mean',\n",
    "    # 'apparent_temperature (°C)': 'mean',\n",
    "    # 'relative_humidity_2m (%)': 'mean',\n",
    "    # 'precipitation_probability (%)': 'max',\n",
    "    # 'showers (mm)': 'sum',\n",
    "    # 'snow_depth (m)': 'max',\n",
    "    # 'pressure_msl (hPa)': 'mean',\n",
    "    # 'surface_pressure (hPa)': 'mean',\n",
    "    # 'visibility (m)': 'min',\n",
    "    # 'evapotranspiration (mm)': 'sum',\n",
    "    # 'et0_fao_evapotranspiration (mm)': 'sum',\n",
    "    'wind_speed_10m (km/h)': 'mean',\n",
    "    # 'wind_speed_80m (km/h)': 'mean',\n",
    "    # 'wind_speed_120m (km/h)': 'mean',\n",
    "    # 'wind_speed_180m (km/h)': 'mean',\n",
    "    # 'wind_gusts_10m (km/h)': 'mean',\n",
    "    # 'temperature_80m (°C)': 'mean',\n",
    "    # 'temperature_120m (°C)': 'mean',\n",
    "    # 'temperature_180m (°C)': 'mean',\n",
    "    # 'soil_temperature_0cm (°C)': 'mean',\n",
    "    # 'soil_temperature_6cm (°C)': 'mean',\n",
    "    # 'soil_temperature_18cm (°C)': 'mean',\n",
    "    # 'soil_moisture_0_to_1cm (m³/m³)': 'mean',\n",
    "    # 'soil_moisture_1_to_3cm (m³/m³)': 'mean',\n",
    "    # 'soil_moisture_3_to_9cm (m³/m³)': 'mean',\n",
    "    # 'cloud_cover (%)': 'mean',\n",
    "    # 'soil_moisture_9_to_27cm (m³/m³)': 'mean'\n",
    "}).reset_index().rename(columns={'date': 'date'})\n",
    "\n",
    "\n",
    "# Rename columns to match the aggregation results\n",
    "daily_df = daily_df.rename(columns={\n",
    "    'temperature_2m (°C)': 'temp_mean',\n",
    "    'weather_code (wmo code)': 'weather_code_mode',\n",
    "    'rain (mm)': 'rain_sum',\n",
    "    # 'snowfall (cm)': 'snowfall_sum',\n",
    "    'precipitation_mm': 'precipitation_mm',\n",
    "    # 'apparent_temperature (°C)': 'apparent_temp_mean',\n",
    "    # 'relative_humidity_2m (%)': 'humidity_mean',\n",
    "    # 'precipitation_probability (%)': 'precip_prob_max',\n",
    "    # 'showers (mm)': 'showers_sum',\n",
    "    # 'snow_depth (m)': 'snow_depth_max',\n",
    "    # 'pressure_msl (hPa)': 'pressure_msl_mean',\n",
    "    # 'surface_pressure (hPa)': 'surface_pressure_mean',\n",
    "    # 'visibility (m)': 'visibility_min',\n",
    "    # 'evapotranspiration (mm)': 'evapotranspiration_sum',\n",
    "    # 'et0_fao_evapotranspiration (mm)': 'et0_evapotranspiration_sum',\n",
    "    'wind_speed_10m (km/h)': 'wind_mean',\n",
    "    # 'wind_speed_80m (km/h)': 'wind_80m_mean',\n",
    "    # 'wind_speed_120m (km/h)': 'wind_120m_mean',\n",
    "    # 'wind_speed_180m (km/h)': 'wind_180m_mean',\n",
    "    # 'wind_gusts_10m (km/h)': 'wind_gusts_mean',\n",
    "    # 'temperature_80m (°C)': 'temp_80m_mean',\n",
    "    # 'temperature_120m (°C)': 'temp_120m_mean',\n",
    "    # 'temperature_180m (°C)': 'temp_180m_mean',\n",
    "    # 'soil_temperature_0cm (°C)': 'soil_temp_0cm_mean',\n",
    "    # 'soil_temperature_6cm (°C)': 'soil_temp_6cm_mean',\n",
    "    # 'soil_temperature_18cm (°C)': 'soil_temp_18cm_mean',\n",
    "    # 'soil_moisture_0_to_1cm (m³/m³)': 'soil_moisture_0_1_mean',\n",
    "    # 'soil_moisture_1_to_3cm (m³/m³)': 'soil_moisture_1_3_mean',\n",
    "    # 'soil_moisture_3_to_9cm (m³/m³)': 'soil_moisture_3_9_mean',\n",
    "    # 'cloud_cover (%)': 'cloud_cover_mean',\n",
    "    # 'soil_moisture_9_to_27cm (m³/m³)': 'soil_moisture_9_27_mean'\n",
    "})\n",
    "\n",
    "# In real use, replace this with actual delay data\n",
    "# Let's simulate a delay if rain > 10mm or wind > 15km/h\n",
    "# Add more weather parameters that might cause construction delays\n",
    "# Delay if: rain_sum > 10mm, wind_mean > 15km/h, precipitation_mm > 0.5,\n",
    "# temp_mean < 2°C or temp_mean > 35°C, or weather_code_mode in severe codes (e.g., 80, 95, 99)\n",
    "severe_weather_codes = [80, 95, 99]  # heavy rain, thunderstorms, etc.\n",
    "\n",
    "daily_df['delay'] = (\n",
    "    (daily_df['rain_sum'] > 10) |\n",
    "    (daily_df['wind_mean'] > 15) |\n",
    "    (daily_df['precipitation_mm'] > 0.5) |\n",
    "    (daily_df['temp_mean'] < 2) |\n",
    "    (daily_df['temp_mean'] > 35) |\n",
    "    (daily_df['weather_code_mode'].isin(severe_weather_codes))\n",
    ").astype(int)\n",
    "\n",
    "print(\"Daily Weather Data with Delays:\")\n",
    "print(daily_df.head())\n",
    "\n",
    "daily_df.to_csv('result/daily_weather_aggregated.csv', index=False)\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "297eae91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task T1 weather delay: 2 days due to precipitation, Rain, and Wind.\n",
      "Task T2 weather delay: 2 days due to precipitation, Rain, and Wind.\n",
      "Task T3 weather delay: 1 days due to precipitation, Rain, and Wind.\n",
      "Task T4 weather delay: 0 days due to precipitation, Rain, and Wind.\n",
      "Task T5 weather delay: 1 days due to precipitation, Rain, and Wind.\n",
      "Task T6 weather delay: 0 days due to precipitation, Rain, and Wind.\n",
      "Updated Project Schedule with Weather Delays:\n",
      "  task_id               task_name new_start_date new_end_date  new_duration\n",
      "0      T1              Excavation     2024-05-01   2024-05-09             9\n",
      "1      T2         Soil Compaction     2024-05-10   2024-05-17             8\n",
      "2      T3         Foundation Pour     2024-05-18   2024-05-29            12\n",
      "3      T4  Basement Waterproofing     2024-05-30   2024-06-03             5\n",
      "4      T5             Backfilling     2024-06-04   2024-06-08             5\n",
      "5      T6            Slab Casting     2024-06-09   2024-06-15             7\n",
      "6      T7     Ground Floor Column     2024-06-16   2024-06-19             4\n",
      "7      T8      Ground Floor Walls     2024-06-20   2024-06-24             5\n",
      "8      T9               Roof Slab     2024-06-25   2024-07-03             9\n",
      "\n",
      "Total Project Delay due to Weather: -4 days\n"
     ]
    }
   ],
   "source": [
    "# Ensure date columns are datetime\n",
    "# tasks_df = pd.read_csv('data/tasks.csv')\n",
    "tasks_df = pd.read_csv('result/updated_tasks.csv')\n",
    "daily_weather_df = pd.read_csv('result/daily_weather_aggregated.csv')\n",
    "\n",
    "tasks_df['start_date'] = pd.to_datetime(tasks_df['start_date'])\n",
    "tasks_df['end_date'] = pd.to_datetime(tasks_df['end_date'])\n",
    "weather_df = weather_history_2024_df\n",
    "weather_df['date'] = pd.to_datetime(weather_df['time'])\n",
    "daily_df['date'] = pd.to_datetime(weather_df['time'])\n",
    "# Ensure daily_weather_df['date'] is datetime for comparison\n",
    "daily_weather_df['date'] = pd.to_datetime(daily_weather_df['date'])\n",
    "# print(tasks_df['start_date'], weather_df['date'])\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "# Function to predict weather delays and adjust task durations based on weather conditions\n",
    "def predict_weather_delays(tasks_df, daily_weather_df):\n",
    "    tasks_df['weather_delay_days'] = 0\n",
    "    for index, task in tasks_df.iterrows():\n",
    "        if task['weather_sensitive'] == 1:\n",
    "            task_weather = daily_weather_df[(daily_weather_df['date'] >= task['start_date']) & \n",
    "                                      (daily_weather_df['date'] <= task['end_date'])]\n",
    "            # print(task_weather)\n",
    "            # delay_days = (((task_weather['precipitation_mm'] >= 2).sum()) / task_weather['precipitation_mm'].count())\n",
    "    #         print(\"hi\",(task_weather['precipitation_mm'] >= 2).sum(),task_weather['precipitation_mm'].count())\n",
    "            delay_days = task_weather['delay'].sum()\n",
    "            print(f\"Task {task['task_id']} weather delay: {delay_days} days due to precipitation, Rain, and Wind.\")\n",
    "            tasks_df.at[index, 'weather_delay_days'] = delay_days\n",
    "    tasks_df['new_duration'] = tasks_df['duration_days'] + tasks_df['weather_delay_days']\n",
    "    return tasks_df\n",
    "\n",
    "# Function to recalculate schedule\n",
    "def recalculate_schedule(tasks_df):\n",
    "    tasks_df['new_start_date'] = pd.NaT\n",
    "    tasks_df['new_end_date'] = pd.NaT\n",
    "    for index, task in tasks_df.iterrows():\n",
    "        if pd.isna(task['depends_on']):\n",
    "            tasks_df.at[index, 'new_start_date'] = task['start_date']\n",
    "        else:\n",
    "            predecessor_end = tasks_df[tasks_df['task_id'] == task['depends_on']]['new_end_date'].iloc[0]\n",
    "            tasks_df.at[index, 'new_start_date'] = predecessor_end + timedelta(days=1)\n",
    "        tasks_df.at[index, 'new_end_date'] = tasks_df.at[index, 'new_start_date'] + timedelta(days=task['new_duration'] - 1)\n",
    "    return tasks_df\n",
    "\n",
    "\n",
    "# Apply weather delays\n",
    "tasks_df = predict_weather_delays(tasks_df, daily_weather_df)\n",
    "\n",
    "# Recalculate schedule\n",
    "tasks_df = recalculate_schedule(tasks_df)\n",
    "\n",
    "# Format dates for output\n",
    "tasks_df['new_start_date'] = tasks_df['new_start_date'].dt.strftime('%Y-%m-%d')\n",
    "tasks_df['new_end_date'] = tasks_df['new_end_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Print updated schedule\n",
    "print(\"Updated Project Schedule with Weather Delays:\")\n",
    "print(tasks_df[['task_id', 'task_name', 'new_start_date', 'new_end_date', 'new_duration']])\n",
    "\n",
    "# Calculate total project delay\n",
    "original_end = tasks_df['end_date'].max()\n",
    "new_end = pd.to_datetime(tasks_df['new_end_date'].max())\n",
    "total_delay = (new_end - original_end).days\n",
    "print(f\"\\nTotal Project Delay due to Weather: {total_delay} days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e89c143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates 52.52000045776367°N 13.419998168945312°E\n",
      "Elevation 38.0 m asl\n",
      "Timezone b'Europe/London'b'GMT+1'\n",
      "Timezone difference to GMT+0 3600 s\n",
      "hi Series([], Name: date, dtype: datetime64[ns, UTC]) -------------------- 0\n",
      "hi Series([], Name: date, dtype: datetime64[ns, UTC]) -------------------- 0\n",
      "hi Series([], Name: date, dtype: datetime64[ns, UTC]) -------------------- 0\n",
      "hi Series([], Name: date, dtype: datetime64[ns, UTC]) -------------------- 0\n",
      "hi Series([], Name: date, dtype: datetime64[ns, UTC]) -------------------- 0\n",
      "hi Series([], Name: date, dtype: datetime64[ns, UTC]) -------------------- 0\n",
      "Updated Project Schedule with Weather Delays:\n",
      "                task_name  duration_days new_start_date new_end_date  \\\n",
      "0              Excavation              7     2025-05-01   2025-05-07   \n",
      "1         Soil Compaction              6     2025-05-08   2025-05-13   \n",
      "2         Foundation Pour              9     2025-05-14   2025-05-22   \n",
      "3  Basement Waterproofing              5     2025-05-23   2025-05-27   \n",
      "4             Backfilling              4     2025-05-28   2025-05-31   \n",
      "5            Slab Casting              6     2025-06-01   2025-06-06   \n",
      "6     Ground Floor Column              4     2025-06-07   2025-06-10   \n",
      "7      Ground Floor Walls              5     2025-06-11   2025-06-15   \n",
      "8               Roof Slab              7     2025-06-16   2025-06-22   \n",
      "\n",
      "   new_duration  \n",
      "0             7  \n",
      "1             6  \n",
      "2             9  \n",
      "3             5  \n",
      "4             4  \n",
      "5             6  \n",
      "6             4  \n",
      "7             5  \n",
      "8             7  \n"
     ]
    }
   ],
   "source": [
    "import openmeteo_requests\n",
    "\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "tasks_df = pd.read_csv('data/tasks_2025.csv')\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# Make sure all required weather variables are listed here\n",
    "# The order of variables in hourly or daily is important to assign them correctly below\n",
    "url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "params = {\n",
    "\t\"latitude\": 52.52,\n",
    "\t\"longitude\": 13.41,\n",
    "\t\"daily\": [\"sunset\", \"sunrise\", \"wind_speed_10m_max\", \"wind_gusts_10m_max\", \"uv_index_max\", \"weather_code\", \"temperature_2m_max\", \"temperature_2m_min\", \"apparent_temperature_max\", \"apparent_temperature_min\", \"daylight_duration\", \"sunshine_duration\", \"uv_index_clear_sky_max\", \"rain_sum\", \"showers_sum\", \"snowfall_sum\", \"precipitation_sum\", \"precipitation_hours\", \"precipitation_probability_max\", \"wind_direction_10m_dominant\", \"shortwave_radiation_sum\", \"et0_fao_evapotranspiration\"],\n",
    "\t\"timezone\": \"Europe/London\"\n",
    "}\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# Process first location. Add a for-loop for multiple locations or weather models\n",
    "response = responses[0]\n",
    "print(f\"Coordinates {response.Latitude()}°N {response.Longitude()}°E\")\n",
    "print(f\"Elevation {response.Elevation()} m asl\")\n",
    "print(f\"Timezone {response.Timezone()}{response.TimezoneAbbreviation()}\")\n",
    "print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
    "\n",
    "# Process daily data. The order of variables needs to be the same as requested.\n",
    "daily = response.Daily()\n",
    "daily_sunset = daily.Variables(0).ValuesInt64AsNumpy()\n",
    "daily_sunrise = daily.Variables(1).ValuesInt64AsNumpy()\n",
    "daily_wind_speed_10m_max = daily.Variables(2).ValuesAsNumpy()\n",
    "daily_wind_gusts_10m_max = daily.Variables(3).ValuesAsNumpy()\n",
    "daily_uv_index_max = daily.Variables(4).ValuesAsNumpy()\n",
    "daily_weather_code = daily.Variables(5).ValuesAsNumpy()\n",
    "daily_temperature_2m_max = daily.Variables(6).ValuesAsNumpy()\n",
    "daily_temperature_2m_min = daily.Variables(7).ValuesAsNumpy()\n",
    "# daily_apparent_temperature_max = daily.Variables(8).ValuesAsNumpy()\n",
    "# daily_apparent_temperature_min = daily.Variables(9).ValuesAsNumpy()\n",
    "# daily_daylight_duration = daily.Variables(10).ValuesAsNumpy()\n",
    "# daily_sunshine_duration = daily.Variables(11).ValuesAsNumpy()\n",
    "# daily_uv_index_clear_sky_max = daily.Variables(12).ValuesAsNumpy()\n",
    "daily_rain_sum = daily.Variables(13).ValuesAsNumpy()\n",
    "daily_showers_sum = daily.Variables(14).ValuesAsNumpy()\n",
    "daily_snowfall_sum = daily.Variables(15).ValuesAsNumpy()\n",
    "daily_precipitation_sum = daily.Variables(16).ValuesAsNumpy()\n",
    "# daily_precipitation_hours = daily.Variables(17).ValuesAsNumpy()\n",
    "# daily_precipitation_probability_max = daily.Variables(18).ValuesAsNumpy()\n",
    "# daily_wind_direction_10m_dominant = daily.Variables(19).ValuesAsNumpy()\n",
    "# daily_shortwave_radiation_sum = daily.Variables(20).ValuesAsNumpy()\n",
    "# daily_et0_fao_evapotranspiration = daily.Variables(21).ValuesAsNumpy()\n",
    "\n",
    "daily_data = {\"date\": pd.date_range(\n",
    "\tstart = pd.to_datetime(daily.Time(), unit = \"s\", utc = True),\n",
    "\tend = pd.to_datetime(daily.TimeEnd(), unit = \"s\", utc = True),\n",
    "\tfreq = pd.Timedelta(seconds = daily.Interval()),\n",
    "\tinclusive = \"left\"\n",
    ")}\n",
    "\n",
    "daily_data[\"sunset\"] = daily_sunset\n",
    "daily_data[\"sunrise\"] = daily_sunrise\n",
    "daily_data[\"wind_speed_10m_max\"] = daily_wind_speed_10m_max\n",
    "daily_data[\"wind_gusts_10m_max\"] = daily_wind_gusts_10m_max\n",
    "daily_data[\"uv_index_max\"] = daily_uv_index_max\n",
    "daily_data[\"weather_code\"] = daily_weather_code\n",
    "daily_data[\"temperature_2m_max\"] = daily_temperature_2m_max\n",
    "daily_data[\"temperature_2m_min\"] = daily_temperature_2m_min\n",
    "# daily_data[\"apparent_temperature_max\"] = daily_apparent_temperature_max\n",
    "# daily_data[\"apparent_temperature_min\"] = daily_apparent_temperature_min\n",
    "# daily_data[\"daylight_duration\"] = daily_daylight_duration\n",
    "# daily_data[\"sunshine_duration\"] = daily_sunshine_duration\n",
    "# daily_data[\"uv_index_clear_sky_max\"] = daily_uv_index_clear_sky_max\n",
    "daily_data[\"rain_sum\"] = daily_rain_sum\n",
    "daily_data[\"showers_sum\"] = daily_showers_sum\n",
    "daily_data[\"snowfall_sum\"] = daily_snowfall_sum\n",
    "daily_data[\"precipitation_sum\"] = daily_precipitation_sum\n",
    "# daily_data[\"precipitation_hours\"] = daily_precipitation_hours\n",
    "# daily_data[\"precipitation_probability_max\"] = daily_precipitation_probability_max\n",
    "# daily_data[\"wind_direction_10m_dominant\"] = daily_wind_direction_10m_dominant\n",
    "# daily_data[\"shortwave_radiation_sum\"] = daily_shortwave_radiation_sum\n",
    "# daily_data[\"et0_fao_evapotranspiration\"] = daily_et0_fao_evapotranspiration\n",
    "\n",
    "daily_dataframe = pd.DataFrame(data = daily_data)\n",
    "daily_dataframe['time'] = pd.to_datetime(daily_dataframe['date'])\n",
    "# print(daily_dataframe)\n",
    "\n",
    "def predict_weather_delays_live(tasks_df, weather_info_df):\n",
    "    tasks_df['weather_delay_days'] = 0\n",
    "    for index, task in tasks_df.iterrows():\n",
    "        if task['weather_sensitive'] == 1:\n",
    "            task_weather = weather_info_df[(weather_info_df['date'] >= task['start_date']) & \n",
    "                                      (weather_info_df['date'] <= task['end_date'])]\n",
    "            # print(task_weather)\n",
    "            delay_days = ((task_weather['precipitation_sum'] >= 2).sum())\n",
    "            print(\"hi\", task_weather['date'],'--------------------', delay_days)\n",
    "            # print(f\"Task {task['task_id']} weather delay: {delay_days} days due to precipitation, Rain, and Wind.\")\n",
    "            tasks_df.at[index, 'weather_delay_days'] = delay_days\n",
    "    tasks_df['new_duration'] = tasks_df['duration_days'] + tasks_df['weather_delay_days']\n",
    "    return tasks_df\n",
    "\n",
    "\n",
    "# Function to recalculate schedule\n",
    "def recalculate_schedule(tasks_df):\n",
    "    tasks_df['new_start_date'] = pd.NaT\n",
    "    tasks_df['new_end_date'] = pd.NaT\n",
    "    for index, task in tasks_df.iterrows():\n",
    "        if pd.isna(task['depends_on']):\n",
    "            tasks_df.at[index, 'new_start_date'] = task['start_date']\n",
    "        else:\n",
    "            predecessor_end = tasks_df[tasks_df['task_id'] == task['depends_on']]['new_end_date'].iloc[0]\n",
    "            tasks_df.at[index, 'new_start_date'] = predecessor_end + timedelta(days=1)\n",
    "        tasks_df.at[index, 'new_end_date'] = tasks_df.at[index, 'new_start_date'] + timedelta(days=task['new_duration'] - 1)\n",
    "    return tasks_df\n",
    "\n",
    "\n",
    "# Apply weather delays\n",
    "tasks_df = predict_weather_delays_live(tasks_df, daily_dataframe)\n",
    "\n",
    "# Recalculate schedule\n",
    "tasks_df = recalculate_schedule(tasks_df)\n",
    "\n",
    "# Format dates for output\n",
    "tasks_df['new_start_date'] = tasks_df['new_start_date'].dt.strftime('%Y-%m-%d')\n",
    "tasks_df['new_end_date'] = tasks_df['new_end_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Print updated schedule\n",
    "print(\"Updated Project Schedule with Weather Delays:\")\n",
    "# print(tasks_df[['task_id', 'task_name','duration_days', 'new_start_date', 'new_end_date', 'new_duration']])\n",
    "print(tasks_df[[ 'task_name','duration_days', 'new_start_date', 'new_end_date', 'new_duration']])\n",
    "\n",
    "# # Calculate total project delay\n",
    "# original_end = tasks_df['end_date'].max()\n",
    "# new_end = pd.to_datetime(tasks_df['new_end_date'].max())\n",
    "# original_end_ts = pd.to_datetime(original_end)\n",
    "# total_delay = (new_end - original_end_ts).days\n",
    "# print(f\"\\nTotal Project Delay due to Weather: {total_delay} days\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d00d860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(         date  temperature_max  temperature_min  rainfall  windspeed  \\\n",
       " 0  2025-05-01             21.5              1.5      4.87        9.3   \n",
       " 1  2025-05-02             13.3              2.5     17.74        2.0   \n",
       " 2  2025-05-03             15.5             14.5      0.86       10.6   \n",
       " 3  2025-05-04             28.5              0.1     12.08        8.3   \n",
       " 4  2025-05-05             22.3              6.6      7.60       21.5   \n",
       " \n",
       "    humidity  weather_code  snowfall  delayed  \n",
       " 0      35.7            45      1.95        1  \n",
       " 1      90.0            61      4.23        0  \n",
       " 2      68.6             3      0.13        1  \n",
       " 3      63.4            80      2.60        0  \n",
       " 4      53.5            95      0.96        0  ,\n",
       " {'0': {'precision': 0.8333333333333334,\n",
       "   'recall': 0.8333333333333334,\n",
       "   'f1-score': 0.8333333333333334,\n",
       "   'support': 6.0},\n",
       "  '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1.0},\n",
       "  'accuracy': 0.7142857142857143,\n",
       "  'macro avg': {'precision': 0.4166666666666667,\n",
       "   'recall': 0.4166666666666667,\n",
       "   'f1-score': 0.4166666666666667,\n",
       "   'support': 7.0},\n",
       "  'weighted avg': {'precision': 0.7142857142857143,\n",
       "   'recall': 0.7142857142857143,\n",
       "   'f1-score': 0.7142857142857143,\n",
       "   'support': 7.0}},\n",
       " {'0': {'precision': 0.8,\n",
       "   'recall': 0.6666666666666666,\n",
       "   'f1-score': 0.7272727272727273,\n",
       "   'support': 6.0},\n",
       "  '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1.0},\n",
       "  'accuracy': 0.5714285714285714,\n",
       "  'macro avg': {'precision': 0.4,\n",
       "   'recall': 0.3333333333333333,\n",
       "   'f1-score': 0.36363636363636365,\n",
       "   'support': 7.0},\n",
       "  'weighted avg': {'precision': 0.6857142857142858,\n",
       "   'recall': 0.5714285714285714,\n",
       "   'f1-score': 0.6233766233766234,\n",
       "   'support': 7.0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate dummy weather data for one month\n",
    "def generate_dummy_weather_data(start_date='2025-05-01', days=31):\n",
    "    data = []\n",
    "    base_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    for i in range(days):\n",
    "        date = base_date + timedelta(days=i)\n",
    "        row = {\n",
    "            'date': date.strftime('%Y-%m-%d'),\n",
    "            'temperature_max': round(np.random.uniform(10, 35), 1),\n",
    "            'temperature_min': round(np.random.uniform(0, 15), 1),\n",
    "            'rainfall': round(np.random.uniform(0, 20), 2),\n",
    "            'windspeed': round(np.random.uniform(0, 25), 1),\n",
    "            'humidity': round(np.random.uniform(30, 100), 1),\n",
    "            'weather_code': random.choice([1, 2, 3, 45, 61, 95, 80]),\n",
    "            'snowfall': round(np.random.uniform(0, 5), 2),\n",
    "            'delayed': random.choice([0, 1])  # 0 = No Delay, 1 = Delay\n",
    "        }\n",
    "        data.append(row)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate data\n",
    "weather_df = generate_dummy_weather_data()\n",
    "# weather_df = weather_history_2024_df\n",
    "\n",
    "# Features and labels\n",
    "features = ['temperature_max', 'temperature_min', 'rainfall', 'windspeed', 'humidity', 'weather_code', 'snowfall']\n",
    "X = weather_df[features]\n",
    "y = weather_df['delayed']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train, y_train)\n",
    "y_pred_log = log_model.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Output results\n",
    "log_report = classification_report(y_test, y_pred_log, output_dict=True)\n",
    "rf_report = classification_report(y_test, y_pred_rf, output_dict=True)\n",
    "\n",
    "weather_df.head(), log_report, rf_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a420916",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d52dbb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34194/785634208.py:12: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  time_index = pd.date_range(start=start_date, end=end_date - timedelta(seconds=1), freq='H')\n",
      "/home/chandan/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.7142857142857143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83         6\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.71         7\n",
      "   macro avg       0.42      0.42      0.42         7\n",
      "weighted avg       0.71      0.71      0.71         7\n",
      "\n",
      "Logistic Regression Coefficients:\n",
      "temperature_2m (°C): 0.3989922259346297\n",
      "rain (mm): -0.7476631074532596\n",
      "snowfall (cm): -0.2753726825933657\n",
      "precipitation (mm): -0.15641620445347068\n",
      "relative_humidity_2m (%): -0.21689749694839813\n",
      "wind_speed_10m (km/h): 0.3224632506847265\n",
      "visibility (m): 0.004274748041129992\n",
      "snow_depth (m): -7.073690999652993e-05\n",
      "Random Forest Accuracy: 0.8571428571428571\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92         6\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.86         7\n",
      "   macro avg       0.43      0.50      0.46         7\n",
      "weighted avg       0.73      0.86      0.79         7\n",
      "\n",
      "Random Forest Feature Importances:\n",
      "temperature_2m (°C): 0.1750190704778437\n",
      "rain (mm): 0.14953858744682227\n",
      "snowfall (cm): 0.24938768571307138\n",
      "precipitation (mm): 0.115952636008737\n",
      "relative_humidity_2m (%): 0.062210657393987394\n",
      "wind_speed_10m (km/h): 0.0727504887526563\n",
      "visibility (m): 0.112832039528396\n",
      "snow_depth (m): 0.06230883467848602\n",
      "\n",
      "Days Predicted as Unsuitable for Construction:\n",
      "            temperature_2m (°C)  rain (mm)  snowfall (cm)  \\\n",
      "time                                                        \n",
      "2024-01-01            -0.003999   5.536754       1.068446   \n",
      "2024-01-02            -0.818254   5.113818       0.975333   \n",
      "2024-01-03             0.208454   4.245036       1.085368   \n",
      "2024-01-04            -0.085581   5.202001       0.779361   \n",
      "2024-01-05             0.150483   5.436036       0.830268   \n",
      "2024-01-06             0.177322   5.558917       1.015257   \n",
      "2024-01-07            -0.571642   4.266081       1.109251   \n",
      "2024-01-08            -0.057948   5.542732       1.027791   \n",
      "2024-01-09             0.760354   4.689925       1.034988   \n",
      "2024-01-10             1.279074   4.162393       0.961209   \n",
      "2024-01-11             0.192326   5.243595       0.887001   \n",
      "2024-01-12             0.407565   5.808607       0.852279   \n",
      "2024-01-13            -0.264444   5.091569       1.185257   \n",
      "2024-01-14            -0.885764   5.262885       0.877316   \n",
      "2024-01-15            -0.151714   5.839018       1.014431   \n",
      "2024-01-16            -0.056444   5.319428       0.793781   \n",
      "2024-01-18            -0.318730   4.143072       1.008335   \n",
      "2024-01-20             0.281882   5.414915       0.838194   \n",
      "2024-01-21            -0.926416   4.175252       0.995643   \n",
      "2024-01-22            -0.047832   5.531980       0.999335   \n",
      "2024-01-23             0.463189   5.411273       0.820554   \n",
      "2024-01-24             0.410685   4.447658       1.010308   \n",
      "2024-01-25            -0.460342   4.694233       1.042358   \n",
      "2024-01-26             0.280202   4.612899       1.166384   \n",
      "2024-01-27             0.787268   5.782576       0.793203   \n",
      "2024-01-28             0.242515   4.538575       1.273324   \n",
      "2024-01-29            -0.630975   4.428923       1.191433   \n",
      "2024-01-30             1.646287   5.958184       0.647197   \n",
      "2024-01-31             1.071547   5.331194       1.068089   \n",
      "\n",
      "            wind_speed_10m (km/h)  visibility (m)  snow_depth (m)  \n",
      "time                                                               \n",
      "2024-01-01              38.396678     1552.971922        0.083894  \n",
      "2024-01-02              38.852958     1174.012553        0.086925  \n",
      "2024-01-03              39.388197     1197.921050        0.089700  \n",
      "2024-01-04              36.642391     1010.768220        0.083766  \n",
      "2024-01-05              36.728548     1059.749173        0.089020  \n",
      "2024-01-06              35.998186     1134.317656        0.088739  \n",
      "2024-01-07              37.300900     1361.572370        0.087290  \n",
      "2024-01-08              39.674177     1729.301987        0.089493  \n",
      "2024-01-09              39.055345     1385.391121        0.089180  \n",
      "2024-01-10              39.141693     1203.360833        0.082854  \n",
      "2024-01-11              36.896803     1216.583168        0.088773  \n",
      "2024-01-12              39.175746     1491.311751        0.089164  \n",
      "2024-01-13              34.048215     1503.562733        0.089287  \n",
      "2024-01-14              38.804358     1049.221308        0.087777  \n",
      "2024-01-15              39.620477     1362.915292        0.089207  \n",
      "2024-01-16              39.139245     2236.821138        0.084519  \n",
      "2024-01-18              36.106156     1005.998853        0.085178  \n",
      "2024-01-20              38.529486     1312.342085        0.088197  \n",
      "2024-01-21              37.677944     1216.632662        0.086446  \n",
      "2024-01-22              39.009205     1539.885449        0.085598  \n",
      "2024-01-23              39.645523     1125.884140        0.086854  \n",
      "2024-01-24              39.580523     1258.698619        0.083616  \n",
      "2024-01-25              39.571420     1167.344893        0.088118  \n",
      "2024-01-26              39.941400     1167.454174        0.084178  \n",
      "2024-01-27              36.486340     1712.694397        0.089146  \n",
      "2024-01-28              38.795641     1689.453419        0.084687  \n",
      "2024-01-29              38.820846     1330.099256        0.081497  \n",
      "2024-01-30              36.484920     2073.880545        0.086018  \n",
      "2024-01-31              39.065907     1758.669045        0.082013  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chandan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/chandan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/chandan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Generate dummy hourly weather data for January 2024\n",
    "start_date = datetime(2024, 1, 1)\n",
    "end_date = start_date + timedelta(days=31)\n",
    "time_index = pd.date_range(start=start_date, end=end_date - timedelta(seconds=1), freq='H')\n",
    "n_rows = len(time_index)\n",
    "\n",
    "data = {\n",
    "    'time': time_index,\n",
    "    'temperature_2m (°C)': np.random.uniform(-5, 5, n_rows),\n",
    "    'weather_code (wmo code)': np.random.randint(0, 100, n_rows),\n",
    "    'rain (mm)': np.random.uniform(0, 5/12, n_rows),  # Adjusted for balance\n",
    "    'snowfall (cm)': np.random.uniform(0, 1/12, n_rows),  # Adjusted for balance\n",
    "    'precipitation (mm)': np.random.uniform(0, 5/12, n_rows),\n",
    "    'apparent_temperature (°C)': np.random.uniform(-5, 5, n_rows),\n",
    "    'relative_humidity_2m (%)': np.random.uniform(60, 100, n_rows),\n",
    "    'precipitation_probability (%)': np.random.uniform(0, 100, n_rows),\n",
    "    'showers (mm)': np.random.uniform(0, 3/12, n_rows),\n",
    "    'snow_depth (m)': np.random.uniform(0, 0.09, n_rows),  # Adjusted to be < 0.1\n",
    "    'pressure_msl (hPa)': np.random.uniform(990, 1010, n_rows),\n",
    "    'surface_pressure (hPa)': np.random.uniform(990, 1010, n_rows),\n",
    "    'visibility (m)': np.random.uniform(1000, 10000, n_rows),  # Adjusted to be > 1000\n",
    "    'evapotranspiration (mm)': np.random.uniform(0, 2, n_rows),\n",
    "    'et0_fao_evapotranspiration (mm)': np.random.uniform(0, 2, n_rows),\n",
    "    'wind_speed_10m (km/h)': np.random.uniform(0, 40, n_rows),  # Adjusted to be < 50\n",
    "    'wind_speed_80m (km/h)': np.random.uniform(0, 50, n_rows),\n",
    "    'wind_speed_120m (km/h)': np.random.uniform(0, 60, n_rows),\n",
    "    'wind_speed_180m (km/h)': np.random.uniform(0, 70, n_rows),\n",
    "    'wind_gusts_10m (km/h)': np.random.uniform(0, 40, n_rows),\n",
    "    'temperature_80m (°C)': np.random.uniform(-5, 5, n_rows),\n",
    "    'temperature_120m (°C)': np.random.uniform(-5, 5, n_rows),\n",
    "    'temperature_180m (°C)': np.random.uniform(-5, 5, n_rows),\n",
    "    'soil_temperature_0cm (°C)': np.random.uniform(0, 5, n_rows),\n",
    "    'soil_temperature_6cm (°C)': np.random.uniform(0, 5, n_rows),\n",
    "    'soil_temperature_18cm (°C)': np.random.uniform(0, 5, n_rows),\n",
    "    'soil_moisture_0_to_1cm (m³/m³)': np.random.uniform(0.2, 0.4, n_rows),\n",
    "    'soil_moisture_1_to_3cm (m³/m³)': np.random.uniform(0.2, 0.4, n_rows),\n",
    "    'soil_moisture_3_to_9cm (m³/m³)': np.random.uniform(0.2, 0.4, n_rows),\n",
    "    'cloud_cover (%)': np.random.uniform(0, 100, n_rows),\n",
    "    'soil_moisture_9_to_27cm (m³/m³)': np.random.uniform(0.2, 0.4, n_rows),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('time', inplace=True)\n",
    "# Step 2: Aggregate hourly data to daily data\n",
    "aggregations = {\n",
    "    'temperature_2m (°C)': 'mean',\n",
    "    'weather_code (wmo code)': 'mean',\n",
    "    'rain (mm)': 'sum',\n",
    "    'snowfall (cm)': 'sum',\n",
    "    'precipitation (mm)': 'sum',\n",
    "    'apparent_temperature (°C)': 'mean',\n",
    "    'relative_humidity_2m (%)': 'mean',\n",
    "    'precipitation_probability (%)': 'mean',\n",
    "    'showers (mm)': 'sum',\n",
    "    'snow_depth (m)': 'max',\n",
    "    'pressure_msl (hPa)': 'mean',\n",
    "    'surface_pressure (hPa)': 'mean',\n",
    "    'visibility (m)': 'min',\n",
    "    'evapotranspiration (mm)': 'sum',\n",
    "    'et0_fao_evapotranspiration (mm)': 'sum',\n",
    "    'wind_speed_10m (km/h)': 'max',\n",
    "    'wind_speed_80m (km/h)': 'max',\n",
    "    'wind_speed_120m (km/h)': 'max',\n",
    "    'wind_speed_180m (km/h)': 'max',\n",
    "    'wind_gusts_10m (km/h)': 'max',\n",
    "    'temperature_80m (°C)': 'mean',\n",
    "    'temperature_120m (°C)': 'mean',\n",
    "    'temperature_180m (°C)': 'mean',\n",
    "    'soil_temperature_0cm (°C)': 'mean',\n",
    "    'soil_temperature_6cm (°C)': 'mean',\n",
    "    'soil_temperature_18cm (°C)': 'mean',\n",
    "    'soil_moisture_0_to_1cm (m³/m³)': 'mean',\n",
    "    'soil_moisture_1_to_3cm (m³/m³)': 'mean',\n",
    "    'soil_moisture_3_to_9cm (m³/m³)': 'mean',\n",
    "    'cloud_cover (%)': 'mean',\n",
    "    'soil_moisture_9_to_27cm (m³/m³)': 'mean',\n",
    "}\n",
    "\n",
    "daily_df = df.resample('D').agg(aggregations)\n",
    "\n",
    "# Step 3: Define the target variable 'suitable'\n",
    "def is_suitable(row):\n",
    "    if row['rain (mm)'] > 5 or \\\n",
    "       row['snowfall (cm)'] > 1 or \\\n",
    "       row['temperature_2m (°C)'] < 0 or row['temperature_2m (°C)'] > 35 or \\\n",
    "       row['wind_speed_10m (km/h)'] > 50 or \\\n",
    "       row['visibility (m)'] < 1000 or \\\n",
    "       row['snow_depth (m)'] > 0.1:\n",
    "        return 0  # unsuitable\n",
    "    else:\n",
    "        return 1  # suitable\n",
    "\n",
    "daily_df['suitable'] = daily_df.apply(is_suitable, axis=1)\n",
    "\n",
    "# Step 4: Select features and target\n",
    "features = [\n",
    "    'temperature_2m (°C)',\n",
    "    'rain (mm)',\n",
    "    'snowfall (cm)',\n",
    "    'precipitation (mm)',\n",
    "    'relative_humidity_2m (%)',\n",
    "    'wind_speed_10m (km/h)',\n",
    "    'visibility (m)',\n",
    "    'snow_depth (m)',\n",
    "]\n",
    "\n",
    "X = daily_df[features]\n",
    "y = daily_df['suitable']\n",
    "\n",
    "# Step 5: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Train logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "print(\"Logistic Regression Coefficients:\")\n",
    "for feature, coef in zip(features, logreg.coef_[0]):\n",
    "    print(f\"{feature}: {coef}\")\n",
    "\n",
    "# Step 7: Train random forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "print(\"Random Forest Feature Importances:\")\n",
    "for feature, importance in zip(features, rf.feature_importances_):\n",
    "    print(f\"{feature}: {importance}\")\n",
    "\n",
    "# Step 8: Identify days predicted as unsuitable\n",
    "daily_df['predicted_suitable'] = rf.predict(X)\n",
    "unsuitable_days = daily_df[daily_df['predicted_suitable'] == 0][['temperature_2m (°C)', 'rain (mm)', 'snowfall (cm)', 'wind_speed_10m (km/h)', 'visibility (m)', 'snow_depth (m)']]\n",
    "print(\"\\nDays Predicted as Unsuitable for Construction:\")\n",
    "print(unsuitable_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d51a6ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['temperature_2m', 'rain', 'snowfall', 'precipitation_probability',\\n       'relative_humidity_2m', 'wind_speed_10m', 'wind_gusts_10m',\\n       'visibility', 'cloud_cover', 'soil_moisture_0_to_1cm', 'delayed'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m\n\u001b[1;32m     14\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature_2m\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoil_moisture_0_to_1cm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     25\u001b[0m ]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 3. Drop rows with missing values (or you can impute them)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdelayed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 4. Split into features and labels\u001b[39;00m\n\u001b[1;32m     31\u001b[0m X \u001b[38;5;241m=\u001b[39m df[selected_features]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['temperature_2m', 'rain', 'snowfall', 'precipitation_probability',\\n       'relative_humidity_2m', 'wind_speed_10m', 'wind_gusts_10m',\\n       'visibility', 'cloud_cover', 'soil_moisture_0_to_1cm', 'delayed'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# 1. Load the data\n",
    "df = pd.read_csv('data/weather_history_2024.csv')\n",
    "\n",
    "# 2. Select relevant features\n",
    "selected_features = [\n",
    "    'temperature_2m',\n",
    "    'rain',\n",
    "    'snowfall',\n",
    "    'precipitation_probability',\n",
    "    'relative_humidity_2m',\n",
    "    'wind_speed_10m',\n",
    "    'wind_gusts_10m',\n",
    "    'visibility',\n",
    "    'cloud_cover',\n",
    "    'soil_moisture_0_to_1cm'\n",
    "]\n",
    "\n",
    "# 3. Drop rows with missing values (or you can impute them)\n",
    "df = df[selected_features + ['delayed']].dropna()\n",
    "\n",
    "# 4. Split into features and labels\n",
    "X = df[selected_features]\n",
    "y = df['delayed']\n",
    "\n",
    "# 5. Normalize features (optional for tree models, good for LR)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 6. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- MODEL OPTION 1: Logistic Regression\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train, y_train)\n",
    "y_pred_log = log_model.predict(X_test)\n",
    "\n",
    "# --- MODEL OPTION 2: Random Forest (More robust for non-linear features)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# 7. Evaluation\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# 8. Confusion Matrix for RF\n",
    "# plt.figure(figsize=(6,4))\n",
    "# sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Blues')\n",
    "# plt.title('Random Forest Confusion Matrix')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
